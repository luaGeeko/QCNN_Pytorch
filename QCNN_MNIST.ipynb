{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVgC1GEGZG2I"
      },
      "outputs": [],
      "source": [
        "!pip install pennylane"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pennylane as qml\n",
        "import torch\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "import pennylane.numpy as np\n",
        "import autograd.numpy as anp\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from skimage.transform import resize\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import List, Tuple, Optional\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "o9YeOUgkZysQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"device being used --- {device}\")"
      ],
      "metadata": {
        "id": "JFztFQGiaNxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the file path within Google Drive\n",
        "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "file_path = f'/content/drive/My Drive/Result/results_{current_time}.json'\n",
        "model_params_file_path = f'/content/drive/My Drive/Result/results_params_{current_time}.json'"
      ],
      "metadata": {
        "id": "6AboNkHfjkJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch Data\n",
        "1. choosing small datasize initally to experiment with the model.\n",
        "2. shuffling then and then making train, validation and test splits"
      ],
      "metadata": {
        "id": "5GfawHhvbWaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# datasize to choose for training, validation and test set\n",
        "train_datasize = 2000\n",
        "test_datasize = 1000\n",
        "# fetch data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# shuffle the training data\n",
        "train_indices = np.random.permutation(len(x_train))\n",
        "x_train = x_train[train_indices]\n",
        "y_train = y_train[train_indices]\n",
        "\n",
        "# shuffle the test data\n",
        "test_indices = np.random.permutation(len(x_test))\n",
        "x_test = x_test[test_indices]\n",
        "y_test = y_test[test_indices]\n",
        "\n",
        "# slice the datasize\n",
        "x_train = x_train[:train_datasize]\n",
        "y_train = y_train[:train_datasize]\n",
        "x_test = x_test[:test_datasize]\n",
        "y_test = y_test[:test_datasize]\n",
        "\n",
        "# count the number of each class in x_train\n",
        "train_class_counts = Counter(y_train)\n",
        "\n",
        "# the class counts\n",
        "print(\"Class counts in x_train:\")\n",
        "for label, count in train_class_counts.items():\n",
        "    print(f\"Class {label}: {count}\")\n",
        "\n",
        "def check_imbalance(class_counts, datasize):\n",
        "  avg_count = datasize / 10\n",
        "  # taking imbalance threshold, 20% of average count\n",
        "  threshold = 0.2 * avg_count\n",
        "  for label, count in class_counts.items():\n",
        "    if abs(count - avg_count) > threshold:\n",
        "      return True, label, count\n",
        "  return False, None, None\n",
        "\n",
        "# check for imbalance in training data\n",
        "is_imbalanced_train, train_imbalanced_class, train_imbalanced_count = check_imbalance(train_class_counts, train_datasize)\n",
        "if is_imbalanced_train:\n",
        "    print(f\"\\nImbalance detected in training data for class {train_imbalanced_class} with count {train_imbalanced_count}\")\n",
        "else:\n",
        "    print(\"\\nNo significant imbalance detected in training data\")\n",
        "# split the training data into training and test sets\n",
        "# X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1667, random_state=42)\n",
        "# print(f\"Data for model --- training: {X_train.shape[0]} validation: {X_val.shape[0]} test: {X_test.shape[0]}\")"
      ],
      "metadata": {
        "id": "xo3iXmp2apT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot some to show\n",
        "fig = px.imshow(x_train[:10, :, :], binary_string=True, facet_col=0, facet_col_wrap=5)\n",
        "fig.show()\n",
        "\n",
        "# normalize the images data\n",
        "X_train, X_test = x_train[..., np.newaxis] / 255.0, x_test[..., np.newaxis] / 255.0\n",
        "Y_train = y_train\n",
        "Y_test = y_test"
      ],
      "metadata": {
        "id": "APVpccaZ57YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Reduction"
      ],
      "metadata": {
        "id": "Gt6AKoqvrlzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf.image.resize(X_train[:], (256, 1)).numpy()\n",
        "X_test = tf.image.resize(X_test[:], (256, 1)).numpy()\n",
        "X_train, X_test = tf.squeeze(X_train).numpy(), tf.squeeze(X_test).numpy()"
      ],
      "metadata": {
        "id": "Ry-DG8_3rttf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup params for circuit training\n",
        "U_params = 15\n",
        "total_params = U_params * 3 + 2 * 3\n",
        "n_wires = 8\n",
        "dev = qml.device(\"default.qubit\", wires=n_wires)\n",
        "\n",
        "# randomly initialize the parameters using numpy, we can try later using xavier uniform\n",
        "params = np.random.randn(total_params, requires_grad=True)\n",
        "\n",
        "# Quantum ciruit to be used for convolution\n",
        "def U_SU4(params, wires):  # 15 params\n",
        "    qml.U3(params[0], params[1], params[2], wires=wires[0])\n",
        "    qml.U3(params[3], params[4], params[5], wires=wires[1])\n",
        "    qml.CNOT(wires=[wires[0], wires[1]])\n",
        "    qml.RY(params[6], wires=wires[0])\n",
        "    qml.RZ(params[7], wires=wires[1])\n",
        "    qml.CNOT(wires=[wires[1], wires[0]])\n",
        "    qml.RY(params[8], wires=wires[0])\n",
        "    qml.CNOT(wires=[wires[0], wires[1]])\n",
        "    qml.U3(params[9], params[10], params[11], wires=wires[0])\n",
        "    qml.U3(params[12], params[13], params[14], wires=wires[1])\n",
        "\n",
        "# Quantum Circuits for Convolutional layers\n",
        "def conv_layer1(U, params):\n",
        "    U(params, wires=[0, 7])\n",
        "    for i in range(0, 8, 2):\n",
        "        U(params, wires=[i, i + 1])\n",
        "    for i in range(1, 7, 2):\n",
        "        U(params, wires=[i, i + 1])\n",
        "\n",
        "def conv_layer2(U, params):\n",
        "    U(params, wires=[0, 6])\n",
        "    U(params, wires=[0, 2])\n",
        "    U(params, wires=[4, 6])\n",
        "    U(params, wires=[2, 4])\n",
        "\n",
        "def conv_layer3(U, params):\n",
        "    U(params, wires=[0, 4])\n",
        "    U(params, wires=[4, 2])\n",
        "\n",
        "# Quantum Circuits for Pooling layers\n",
        "def pooling_layer1(V, params):\n",
        "    for i in range(0, 8, 2):\n",
        "        V(params, wires=[i + 1, i])\n",
        "\n",
        "def pooling_layer2(V, params):\n",
        "    V(params, wires=[2, 0])\n",
        "    V(params, wires=[6, 4])\n",
        "\n",
        "def Pooling_ansatz(params, wires):  # 2 params\n",
        "    qml.CRZ(params[0], wires=[wires[0], wires[1]])\n",
        "    qml.PauliX(wires=wires[0])\n",
        "    qml.CRX(params[1], wires=[wires[0], wires[1]])\n",
        "\n",
        "# define circuit layers\n",
        "def QCNN_structure_modified(U, params, U_params):\n",
        "    param1 = params[0:U_params]  # 15 params\n",
        "    param2 = params[U_params:2 * U_params]  # 15 params\n",
        "    param3 = params[2 * U_params:3 * U_params]  # 15 params\n",
        "    param4 = params[3 * U_params:3 * U_params + 2]  # 2 params\n",
        "    param5 = params[3 * U_params + 2:3 * U_params + 4]  # 2 params\n",
        "    #param6 = params[3 * U_params + 4:3 * U_params + 6]  # 2 params\n",
        "\n",
        "    # print(f\"total params -- {params.shape} U_params {U_params}\")\n",
        "    # print(\n",
        "    #     f\"Params: {param1.shape} {param2.shape} {param3.shape} {param4.shape} {param5.shape}\"\n",
        "    # )\n",
        "    # layer 1\n",
        "    conv_layer1(U, param1)\n",
        "    pooling_layer1(Pooling_ansatz, param4)\n",
        "    # layer 2\n",
        "    conv_layer2(U, param2)\n",
        "    pooling_layer2(Pooling_ansatz, param5)\n",
        "    # layer 3\n",
        "    conv_layer3(U, param3)\n",
        "\n",
        "# define circuit\n",
        "@qml.qnode(dev)\n",
        "def QCNN(X, params, U_params, embedding_type=\"Amplitude\", cost_fn=\"cross_entropy\"):\n",
        "  # data encoding\n",
        "  qml.AmplitudeEmbedding(X, wires=range(8), normalize=True)\n",
        "  # circuit with params\n",
        "  QCNN_structure_modified(U_SU4, params, U_params=U_params)\n",
        "\n",
        "  # compute cost_fun\n",
        "  result_16_states = qml.probs(wires=[0, 1, 2, 3])\n",
        "  return result_16_states\n",
        "\n",
        "# draw the circuit\n",
        "x_sample = X_test[0].reshape(1, X_test[0].shape[0])\n",
        "print(qml.draw_mpl(QCNN)(x_sample, params, U_params))"
      ],
      "metadata": {
        "id": "67V_WuYT67sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define loss and accuracy functions\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "def multi_class_cross_entropy(labels: List, predictions: List, num_classes: Optional[int] = 10):\n",
        "    \"\"\"\n",
        "    Compute the cross-entropy loss between ground truth labels and predicted probabilities.\n",
        "\n",
        "    Args:\n",
        "    - labels (array): Ground truth labels, shape (num_samples,)\n",
        "    - predictions (array): Predicted probabilities for each class, shape (num_samples, num_classes)\n",
        "\n",
        "    Returns:\n",
        "    - loss (float): Cross-entropy loss\n",
        "    \"\"\"\n",
        "    num_samples = len(labels)\n",
        "    loss = 0\n",
        "    for i in range(num_samples):\n",
        "        label = labels[i]\n",
        "        prediction = predictions[i]\n",
        "        # FIXME: as the probabilites from qml are not in between 0 and 1, we need to normalize them apply softmax function\n",
        "        softmax_probabilites = softmax(prediction)\n",
        "        # testing the predicted class label\n",
        "        # predicted_class_label = np.argmax(softmax_probabilites)\n",
        "        c_entropy = -anp.log(softmax_probabilites[label])\n",
        "        loss += c_entropy\n",
        "    return loss / num_samples\n",
        "\n",
        "def cost(params, X, Y, U_params, embedding_type=\"Amplitude\", cost_fn=\"cross_entropy\"):\n",
        "    # we need predictions for 10 classes only\n",
        "    predictions = [QCNN(x, params, U_params, embedding_type=embedding_type, cost_fn=cost_fn)[:10] for x in X]\n",
        "    loss = multi_class_cross_entropy(Y, predictions)\n",
        "    return loss\n",
        "\n",
        "def get_predicted_labels_QCNN(params, X, Y, U_params, embedding_type=\"Amplitude\", cost_fn=\"cross_entropy\"):\n",
        "    # we need predictions for 10 classes only\n",
        "    predictions = [QCNN(x, params, U_params, embedding_type=embedding_type, cost_fn=cost_fn)[:10] for x in X]\n",
        "    softmax_predictions = [softmax(p) for p in predictions]\n",
        "    # get predicted labels\n",
        "    predicted_labels = np.argmax(softmax_predictions, axis=1)\n",
        "    return predicted_labels\n",
        "\n",
        "def accuracy_test(predictions, labels):\n",
        "  acc = 0\n",
        "  for label, pred in zip(labels, predictions):\n",
        "      if np.argmax(pred) == label:\n",
        "          acc = acc + 1\n",
        "  return acc / len(labels)"
      ],
      "metadata": {
        "id": "If8furm0B1mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparamters\n",
        "n_epochs = 30\n",
        "batch_size = 64\n",
        "initial_lr = 0.01\n",
        "patience = 2\n",
        "lr_factor = 0.1\n",
        "min_lr = 1e-6\n",
        "\n",
        "opt = qml.NesterovMomentumOptimizer(stepsize=initial_lr)\n",
        "\n",
        "tr_steps_per_epoch = len(X_train) // batch_size\n",
        "#val_steps_per_epoch = len(X_val) // batch_size\n",
        "\n",
        "train_loss_history = []\n",
        "train_acc_history = []\n",
        "val_loss_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "best_tra_loss = float('inf')\n",
        "best_train_acc = 0\n",
        "epochs_no_improve = 0\n",
        "# track the current learning rate\n",
        "current_lr = initial_lr\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total_samples = 0\n",
        "  total_loss = 0\n",
        "  correct_count = 0\n",
        "  # shuffle the data for each epoch\n",
        "  indices = np.random.permutation(len(X_train))\n",
        "  X_train_shuffled = X_train[indices]\n",
        "  Y_train_shuffled = Y_train[indices]\n",
        "  for step in range(tr_steps_per_epoch):\n",
        "      # create mini-batch\n",
        "      X_batch = X_train_shuffled[step * batch_size: (step + 1) * batch_size]\n",
        "      Y_batch = Y_train_shuffled[step * batch_size: (step + 1) * batch_size]\n",
        "      prev_params = params\n",
        "      params, cost_new = opt.step_and_cost(\n",
        "            lambda v: cost(v, X_batch, Y_batch, U_params,),\n",
        "            params,\n",
        "        )\n",
        "      predicted_labels = get_predicted_labels_QCNN(prev_params, X_batch, Y_batch, U_params=U_params)\n",
        "      correct_count += np.sum(predicted_labels == Y_batch)\n",
        "      total_samples += len(Y_batch)\n",
        "      total_loss += cost_new * len(Y_batch)\n",
        "  # average accuracy for the epoch\n",
        "  train_accuracy = correct_count / total_samples\n",
        "  # average loss for the epoch\n",
        "  train_average_loss = total_loss / total_samples\n",
        "  train_loss_history.append(train_average_loss)\n",
        "  if isinstance(train_accuracy, qml.numpy.tensor):\n",
        "      train_accuracy = train_accuracy.item()\n",
        "  train_acc_history.append(train_accuracy)\n",
        "  # log training details\n",
        "  print(f\"Epoch {epoch + 1}/{n_epochs}, Average Loss: {train_average_loss:.4f}\")\n",
        "  print(f\"Epoch {epoch + 1}/{n_epochs}, Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "  # Check for improvement in training accuracy\n",
        "  if train_accuracy > best_train_acc:\n",
        "    best_train_acc = train_accuracy\n",
        "    epochs_no_improve = 0\n",
        "  else:\n",
        "    epochs_no_improve += 1\n",
        "\n",
        "  # Reduce learning rate if no improvement for 'patience' epochs\n",
        "  if epochs_no_improve >= patience:\n",
        "    current_lr = max(current_lr * lr_factor, min_lr)  # Ensure learning rate does not go below min_lr\n",
        "    opt = qml.NesterovMomentumOptimizer(stepsize=current_lr)  # Update optimizer with the new learning rate\n",
        "    epochs_no_improve = 0  # Reset patience counter\n",
        "    print(f\"Reducing learning rate to {current_lr}\")\n",
        "\n",
        "  # validation phase\n",
        "  # val_total_samples = 0\n",
        "  # val_total_loss = 0\n",
        "  # val_correct_count = 0\n",
        "  # for step in range(val_steps_per_epoch):\n",
        "  #     # Create mini-batch\n",
        "  #     X_val_batch = X_val[step * batch_size: (step + 1) * batch_size]\n",
        "  #     Y_val_batch = Y_val[step * batch_size: (step + 1) * batch_size]\n",
        "  #     val_cost_new = cost(params, X_val_batch, Y_val_batch, U_params)\n",
        "  #     val_predicted_labels = get_predicted_labels_QCNN(params, X_val_batch, Y_val_batch, U_params=U_params)\n",
        "  #     val_correct_count += np.sum(val_predicted_labels == Y_val_batch)\n",
        "  #     val_total_samples += len(Y_val_batch)\n",
        "  #     val_total_loss += val_cost_new * len(Y_val_batch)\n",
        "  # val_accuracy = val_correct_count / val_total_samples\n",
        "  # val_average_loss = val_total_loss / val_total_samples\n",
        "  # if isinstance(val_average_loss, qml.numpy.tensor):\n",
        "  #     val_average_loss = val_average_loss.item()\n",
        "  # if isinstance(val_accuracy, qml.numpy.tensor):\n",
        "  #     val_accuracy = val_accuracy.item()\n",
        "  # val_loss_history.append(val_average_loss)\n",
        "  # val_acc_history.append(val_accuracy)\n",
        "  # # log validation details\n",
        "  # print(f\"Epoch {epoch + 1}/{n_epochs}, Validation Loss: {val_average_loss:.4f}\")\n",
        "  # print(f\"Epoch {epoch + 1}/{n_epochs}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "  # lets try doing after every 10 epochs\n",
        "  # if (epoch + 1) % 10 == 0:\n",
        "  #   # reduce the current learning rate\n",
        "  #   current_lr *= lr_factor\n",
        "  #   opt = qml.NesterovMomentumOptimizer(stepsize=current_lr)\n",
        "  #   print(f\"Reducing learning rate to {current_lr}\")\n",
        "\n",
        "  # Learning rate scheduling\n",
        "  # if val_average_loss < best_val_loss:\n",
        "  #     best_val_loss = val_average_loss\n",
        "  #     epochs_no_improve = 0\n",
        "  # else:\n",
        "  #     epochs_no_improve += 1\n",
        "\n",
        "  # if epochs_no_improve >= patience:\n",
        "  #   new_lr = max(current_lr * lr_factor, min_lr)\n",
        "  #   if new_lr < current_lr:\n",
        "  #       print(f\"Reducing learning rate from {current_lr} to {new_lr}\")\n",
        "  #       current_lr = new_lr\n",
        "  #       opt = qml.AdamOptimizer(stepsize=current_lr)\n",
        "  #   epochs_no_improve = 0"
      ],
      "metadata": {
        "id": "t1EpZefH_tku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_data = params.tolist()"
      ],
      "metadata": {
        "id": "s3x_wVrxNiHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model params to file\n",
        "if not os.path.exists(os.path.dirname(model_params_file_path)):\n",
        "  os.makedirs(os.path.dirname(model_params_file_path))\n",
        "with open(model_params_file_path, 'w') as loss_f:\n",
        "    json.dump(params_data, loss_f, indent=4)\n",
        "print(f\"model params saved to {model_params_file_path}\")"
      ],
      "metadata": {
        "id": "Zbgyos60M30D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the paramsfile or data file if you want to plot the loss and acc plots\n",
        "training_data_file_path = '/content/drive/My Drive/Result/results_params_20240603_222522.json'\n",
        "with open(training_data_file_path, 'r') as f:\n",
        "  loaded_params = json.load(f)"
      ],
      "metadata": {
        "id": "sZajg6GQME1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run on test set\n",
        "test_predictions = [QCNN(x, loaded_params, U_params)[:10] for x in X_test[:2]]\n",
        "# predictions are to be converted to softmax probabilities\n",
        "softmax_predictions = [softmax(p) for p in test_predictions]\n",
        "test_accuracy = accuracy_test(softmax_predictions, Y_test)\n",
        "print(f\"Test Accuracy {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "GxdM9xNLwk48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_nan_tensors(tensor_list):\n",
        "  clean_tensors = []\n",
        "  nan_indices = []\n",
        "  for i, tensor in enumerate(tensor_list):\n",
        "    if np.isnan(tensor).any().item():\n",
        "      nan_indices.append(i)\n",
        "    else:\n",
        "      clean_tensors.append(tensor)\n",
        "  return clean_tensors, nan_indices\n",
        "\n",
        "# clean the list of tensors and get indices of tensors with NaNs\n",
        "clean_tensors, nan_indices = remove_nan_tensors(softmax_predictions)\n",
        "if nan_indices:\n",
        "  print(f\"found some indices to be nan {len(nan_indices)}\")\n",
        "  # remove those indexs from the list of test image and its label\n",
        "  for index in sorted(nan_indices, reverse=True):\n",
        "    softmax_predictions.pop(index)\n",
        "    Y_test.pop(index)"
      ],
      "metadata": {
        "id": "X1k1EjT01mRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the confusion matrix\n",
        "predicted_test_labels = np.array([np.argmax(pred) for pred in softmax_predictions])\n",
        "cm = confusion_matrix(Y_test[:2], predicted_test_labels)\n",
        "all_labels = np.unique(np.concatenate((Y_test[:2], predicted_test_labels)))\n",
        "# Plot the confusion matrix using sklearn's ConfusionMatrixDisplay\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=all_labels)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cvESc5ZWU3Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save results to drive\n",
        "def convert_to_float(lst):\n",
        "    return [float(item) for item in lst]\n",
        "\n",
        "data = {\n",
        "    \"train_loss\": convert_to_float(train_loss_history),\n",
        "    \"train_acc\": convert_to_float(train_acc_history),\n",
        "    \"val_loss\": convert_to_float(val_loss_history),\n",
        "    \"val_acc\": convert_to_float(val_acc_history),\n",
        "}\n",
        "\n",
        "# Check if the directory exists, if not, create it\n",
        "if not os.path.exists(os.path.dirname(file_path)):\n",
        "  os.makedirs(os.path.dirname(file_path))\n",
        "with open(file_path, 'w') as loss_f:\n",
        "    json.dump(data, loss_f, indent=4)\n",
        "print(f\"Data saved to {file_path}\")"
      ],
      "metadata": {
        "id": "WvxeOFThkXlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_QCNN_metric(data, title, y_axis_title, name):\n",
        "    # Create the plot\n",
        "    fig = go.Figure()\n",
        "    # Add the data trace\n",
        "    fig.add_trace(go.Scatter(y=data, mode='lines', name=name))\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        xaxis_title='Epoch',\n",
        "        yaxis_title=y_axis_title,\n",
        "        legend_title='Legend'\n",
        "    )\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "IsUHwTtfEy8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_QCNN_metric(data=train_loss_history, title='Training Loss Over Epochs', y_axis_title='Loss', name='Training Loss')"
      ],
      "metadata": {
        "id": "YeCvwtfI6GFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_QCNN_metric(data=train_acc_history, title='Training Acc Over Epochs', y_axis_title='Acc', name='Training Acc')"
      ],
      "metadata": {
        "id": "XT6D8-3Z6XHe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
